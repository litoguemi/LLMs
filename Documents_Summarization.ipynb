{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1c3b4e8b-269c-4cc8-8470-1db4a91b6c34",
      "metadata": {
        "id": "1c3b4e8b-269c-4cc8-8470-1db4a91b6c34"
      },
      "source": [
        "<br>\n",
        "\n",
        "# <font color=\"#76b900\">Working with Large Documents</font>\n",
        "\n",
        "\n",
        "### **Environment Setup:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9214bd93-d65d-4dbd-94e3-254a2f670c52",
      "metadata": {
        "id": "9214bd93-d65d-4dbd-94e3-254a2f670c52"
      },
      "outputs": [],
      "source": [
        "%pip install -qq langchain langchain-nvidia-ai-endpoints gradio\n",
        "%pip install -qq arxiv pymupdf\n",
        "\n",
        "import os\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"xxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "\n",
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color=\"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=base_style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c33c07-19b8-4c81-8d99-30fa2b3b2017",
      "metadata": {
        "id": "55c33c07-19b8-4c81-8d99-30fa2b3b2017",
        "outputId": "88299a3d-2e83-4ebb-b396-d411a6679f74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Model(id='01-ai/yi-large', model_type=None),\n",
              " Model(id='adept/fuyu-8b', model_type=None),\n",
              " Model(id='ai21labs/jamba-1.5-large-instruct', model_type=None),\n",
              " Model(id='ai21labs/jamba-1.5-mini-instruct', model_type=None),\n",
              " Model(id='aisingapore/sea-lion-7b-instruct', model_type=None),\n",
              " Model(id='baai/bge-m3', model_type=None),\n",
              " Model(id='baichuan-inc/baichuan2-13b-chat', model_type=None),\n",
              " Model(id='bigcode/starcoder2-15b', model_type=None),\n",
              " Model(id='bigcode/starcoder2-7b', model_type=None),\n",
              " Model(id='deepseek-ai/deepseek-coder-6.7b-instruct', model_type=None),\n",
              " Model(id='google/codegemma-1.1-7b', model_type=None),\n",
              " Model(id='google/deplot', model_type=None),\n",
              " Model(id='google/gemma-2-27b-it', model_type=None),\n",
              " Model(id='google/gemma-2-2b-it', model_type=None),\n",
              " Model(id='google/gemma-2-9b-it', model_type=None),\n",
              " Model(id='google/paligemma', model_type=None),\n",
              " Model(id='google/shieldgemma-9b', model_type=None),\n",
              " Model(id='ibm/granite-34b-code-instruct', model_type=None),\n",
              " Model(id='ibm/granite-8b-code-instruct', model_type=None),\n",
              " Model(id='liuhaotian/llava-v1.6-34b', model_type=None),\n",
              " Model(id='liuhaotian/llava-v1.6-mistral-7b', model_type=None),\n",
              " Model(id='mediatek/breeze-7b-instruct', model_type=None),\n",
              " Model(id='meta/llama-3.1-405b-instruct', model_type=None),\n",
              " Model(id='microsoft/kosmos-2', model_type=None),\n",
              " Model(id='microsoft/phi-3-medium-128k-instruct', model_type=None),\n",
              " Model(id='microsoft/phi-3-medium-4k-instruct', model_type=None),\n",
              " Model(id='microsoft/phi-3-small-128k-instruct', model_type=None),\n",
              " Model(id='microsoft/phi-3-small-8k-instruct', model_type=None),\n",
              " Model(id='microsoft/phi-3-vision-128k-instruct', model_type=None),\n",
              " Model(id='microsoft/phi-3.5-mini-instruct', model_type=None),\n",
              " Model(id='microsoft/phi-3.5-moe-instruct', model_type=None),\n",
              " Model(id='microsoft/phi-3.5-vision-instruct', model_type=None),\n",
              " Model(id='mistralai/codestral-22b-instruct-v0.1', model_type=None),\n",
              " Model(id='mistralai/mamba-codestral-7b-v0.1', model_type=None),\n",
              " Model(id='mistralai/mathstral-7b-v0.1', model_type=None),\n",
              " Model(id='mistralai/mistral-7b-instruct-v0.3', model_type=None),\n",
              " Model(id='mistralai/mistral-large-2-instruct', model_type=None),\n",
              " Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type=None),\n",
              " Model(id='nvidia/llama3-chatqa-1.5-70b', model_type=None),\n",
              " Model(id='nvidia/llama3-chatqa-1.5-8b', model_type=None),\n",
              " Model(id='nvidia/mistral-nemo-minitron-8b-base', model_type=None),\n",
              " Model(id='nvidia/nemotron-4-340b-instruct', model_type=None),\n",
              " Model(id='nvidia/nemotron-4-340b-reward', model_type=None),\n",
              " Model(id='nvidia/nemotron-mini-4b-instruct', model_type=None),\n",
              " Model(id='nvidia/neva-22b', model_type=None),\n",
              " Model(id='nvidia/nvclip', model_type=None),\n",
              " Model(id='nvidia/usdcode-llama3-70b-instruct', model_type=None),\n",
              " Model(id='rakuten/rakutenai-7b-chat', model_type=None),\n",
              " Model(id='rakuten/rakutenai-7b-instruct', model_type=None),\n",
              " Model(id='seallms/seallm-7b-v2.5', model_type=None),\n",
              " Model(id='thudm/chatglm3-6b', model_type=None),\n",
              " Model(id='tokyotech-llm/llama-3-swallow-70b-instruct-v0.1', model_type=None),\n",
              " Model(id='upstage/solar-10.7b-instruct', model_type=None),\n",
              " Model(id='writer/palmyra-fin-70b-32k', model_type=None),\n",
              " Model(id='writer/palmyra-med-70b-32k', model_type=None),\n",
              " Model(id='writer/palmyra-med-70b', model_type=None),\n",
              " Model(id='yentinglin/llama-3-taiwan-70b-instruct', model_type=None),\n",
              " Model(id='databricks/dbrx-instruct', model_type='chat'),\n",
              " Model(id='google/codegemma-7b', model_type='chat'),\n",
              " Model(id='google/gemma-2b', model_type='chat'),\n",
              " Model(id='google/gemma-7b', model_type='chat'),\n",
              " Model(id='google/recurrentgemma-2b', model_type='chat'),\n",
              " Model(id='meta/codellama-70b', model_type='chat'),\n",
              " Model(id='meta/llama-3.1-70b-instruct', model_type='chat'),\n",
              " Model(id='meta/llama-3.1-8b-instruct', model_type='chat'),\n",
              " Model(id='meta/llama2-70b', model_type='chat'),\n",
              " Model(id='meta/llama3-70b-instruct', model_type='chat'),\n",
              " Model(id='meta/llama3-8b-instruct', model_type='chat'),\n",
              " Model(id='microsoft/phi-3-mini-128k-instruct', model_type='chat'),\n",
              " Model(id='microsoft/phi-3-mini-4k-instruct', model_type='chat'),\n",
              " Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat'),\n",
              " Model(id='mistralai/mistral-large', model_type='chat'),\n",
              " Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat'),\n",
              " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat'),\n",
              " Model(id='snowflake/arctic', model_type='chat')]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "ChatNVIDIA.get_available_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de8583a1-c10a-41da-8256-49520f868670",
      "metadata": {
        "id": "de8583a1-c10a-41da-8256-49520f868670"
      },
      "outputs": [],
      "source": [
        "## Useful utility method for printing intermediate states\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from functools import partial\n",
        "\n",
        "def RPrint(preface=\"State: \"):\n",
        "    def print_and_return(x, preface=\"\"):\n",
        "        print(f\"{preface}{x}\")\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
        "\n",
        "def PPrint(preface=\"State: \"):\n",
        "    def print_and_return(x, preface=\"\"):\n",
        "        pprint(preface, x)\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3310462b-f215-4d00-9d59-e613921bed0a",
      "metadata": {
        "id": "3310462b-f215-4d00-9d59-e613921bed0a"
      },
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## Loading Documents\n",
        "\n",
        "By default using `ArxivLoader` to load in one of either the [MRKL](https://arxiv.org/abs/2205.00445) or [ReAct](https://arxiv.org/abs/2210.03629) publication papers as you're likely to run into them at some point in your continued chat model research endeavors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4382b61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4382b61",
        "outputId": "d6e95b9b-97be-4984-a9fd-58a528091146"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 409 ms, sys: 71.1 ms, total: 480 ms\n",
            "Wall time: 768 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "## Loading in the file\n",
        "\n",
        "## Unstructured File Loader: Good for arbitrary \"probably good enough\" loader\n",
        "# documents = UnstructuredFileLoader(\"llama2_paper.pdf\").load()\n",
        "\n",
        "## More specialized loader, won't work for everything, but simple API and usually better results\n",
        "documents = ArxivLoader(query=\"2404.16130\").load()  ## GraphRAG\n",
        "# documents = ArxivLoader(query=\"2404.03622\").load()  ## Visualization-of-Thought\n",
        "# documents = ArxivLoader(query=\"2404.19756\").load()  ## KAN: Kolmogorov-Arnold Networks\n",
        "# documents = ArxivLoader(query=\"2404.07143\").load()  ## Infini-Attention\n",
        "# documents = ArxivLoader(query=\"2210.03629\").load()  ## ReAct"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hw0SL--6cirp",
      "metadata": {
        "id": "hw0SL--6cirp"
      },
      "source": [
        "<br>\n",
        "\n",
        "We can see from our import that we this connector gives us access to two different components:\n",
        "- The `page_content` is the actual body of the document in some human-interpretable format.\n",
        "- The `metadata` is relevant information about the document that is provided by the connector via its data source.\n",
        "\n",
        "Below, we can check out the length of our document body to see what's inside, and will probably notice an intractable document length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2289d525-2c2b-4a99-9a48-00f9b951ae02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2289d525-2c2b-4a99-9a48-00f9b951ae02",
        "outputId": "98b9ef68-c36b-478f-9bbb-1e45b2c49d60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Documents Retrieved: 1\n",
            "Sample of Document 1 Content (Total Length: 53880):\n",
            "From Local to Global: A Graph RAG Approach to\n",
            "Query-Focused Summarization\n",
            "Darren Edge1†\n",
            "Ha Trinh1†\n",
            "Newman Cheng2\n",
            "Joshua Bradley2\n",
            "Alex Chao3\n",
            "Apurva Mody3\n",
            "Steven Truitt2\n",
            "Jonathan Larson1\n",
            "1Microsoft Research\n",
            "2Microsoft Strategic Missions and Technologies\n",
            "3Microsoft Office of the CTO\n",
            "{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,steventruitt,jolarso}\n",
            "@microsoft.com\n",
            "†These authors contributed equally to this work\n",
            "Abstract\n",
            "The use of retrieval-augmented generation (RAG) to retrieve relevant informa-\n",
            "tion from an external knowledge source enables large language models (LLMs)\n",
            "to answer questions over private and/or previously unseen document collections.\n",
            "However, RAG fails on global questions directed at an entire text corpus, such\n",
            "as “What are the main themes in the dataset?”, since this is inherently a query-\n",
            "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
            "QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical\n",
            "RAG systems. \n"
          ]
        }
      ],
      "source": [
        "## Printing out a sample of the content\n",
        "print(\"Number of Documents Retrieved:\", len(documents))\n",
        "print(f\"Sample of Document 1 Content (Total Length: {len(documents[0].page_content)}):\")\n",
        "print(documents[0].page_content[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1JjUK2ZSd0HL",
      "metadata": {
        "id": "1JjUK2ZSd0HL"
      },
      "source": [
        "<br>\n",
        "\n",
        "In contrast, the metadata will be much more conservatively-sized to the point of being viable context components for your favorite chat model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Py2lbRXlcX81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py2lbRXlcX81",
        "outputId": "07197dd4-1609-4ecf-ae54-cf6ef3d25458"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'From Local to Global: A Graph RAG Approach to Query-Focused Summarization'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Jonathan Larson'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The use of retrieval-augmented generation (RAG) to retrieve relevant\\ninformation from an external </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge source enables large language models\\n(LLMs) to answer questions over private and/or previously unseen </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">document\\ncollections. However, RAG fails on global questions directed at an entire text\\ncorpus, such as \"What are</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the main themes in the dataset?\", since this is\\ninherently a query-focused summarization (QFS) task, rather than </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">an explicit\\nretrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities\\nof text indexed by </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">typical RAG systems. To combine the strengths of these\\ncontrasting methods, we propose a Graph RAG approach to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">question answering over\\nprivate text corpora that scales with both the generality of user questions and\\nthe </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">quantity of source text to be indexed. Our approach uses an LLM to build a\\ngraph-based text index in two stages: </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">first to derive an entity knowledge graph\\nfrom the source documents, then to pregenerate community summaries for </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">all\\ngroups of closely-related entities. Given a question, each community summary is\\nused to generate a partial </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">response, before all partial responses are again\\nsummarized in a final response to the user. For a class of global</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">sensemaking\\nquestions over datasets in the 1 million token range, we show that Graph RAG\\nleads to substantial </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">improvements over a na\\\\\"ive RAG baseline for both the\\ncomprehensiveness and diversity of generated answers. An </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">open-source,\\nPython-based implementation of both global and local Graph RAG approaches is\\nforthcoming at </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">https://aka.ms/graphrag.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2024-04-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'From Local to Global: A Graph RAG Approach to Query-Focused Summarization'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, \u001b[0m\n",
              "\u001b[32mJonathan Larson'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The use of retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to retrieve relevant\\ninformation from an external \u001b[0m\n",
              "\u001b[32mknowledge source enables large language models\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to answer questions over private and/or previously unseen \u001b[0m\n",
              "\u001b[32mdocument\\ncollections. However, RAG fails on global questions directed at an entire text\\ncorpus, such as \"What are\u001b[0m\n",
              "\u001b[32mthe main themes in the dataset?\", since this is\\ninherently a query-focused summarization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m task, rather than \u001b[0m\n",
              "\u001b[32man explicit\\nretrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities\\nof text indexed by \u001b[0m\n",
              "\u001b[32mtypical RAG systems. To combine the strengths of these\\ncontrasting methods, we propose a Graph RAG approach to \u001b[0m\n",
              "\u001b[32mquestion answering over\\nprivate text corpora that scales with both the generality of user questions and\\nthe \u001b[0m\n",
              "\u001b[32mquantity of source text to be indexed. Our approach uses an LLM to build a\\ngraph-based text index in two stages: \u001b[0m\n",
              "\u001b[32mfirst to derive an entity knowledge graph\\nfrom the source documents, then to pregenerate community summaries for \u001b[0m\n",
              "\u001b[32mall\\ngroups of closely-related entities. Given a question, each community summary is\\nused to generate a partial \u001b[0m\n",
              "\u001b[32mresponse, before all partial responses are again\\nsummarized in a final response to the user. For a class of global\u001b[0m\n",
              "\u001b[32msensemaking\\nquestions over datasets in the 1 million token range, we show that Graph RAG\\nleads to substantial \u001b[0m\n",
              "\u001b[32mimprovements over a na\\\\\"ive RAG baseline for both the\\ncomprehensiveness and diversity of generated answers. An \u001b[0m\n",
              "\u001b[32mopen-source,\\nPython-based implementation of both global and local Graph RAG approaches is\\nforthcoming at \u001b[0m\n",
              "\u001b[32mhttps://aka.ms/graphrag.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pprint(documents[0].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e0449e4",
      "metadata": {
        "id": "4e0449e4"
      },
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## Transforming The Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f564ee4-262e-4721-bf6b-ee8ebdb7a1ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f564ee4-262e-4721-bf6b-ee8ebdb7a1ba",
        "outputId": "a4e666e5-5a5c-413b-f5a4-acca742d80d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1200,\n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
        ")\n",
        "\n",
        "## Some nice custom preprocessing\n",
        "# documents[0].page_content = documents[0].page_content.replace(\". .\", \"\")\n",
        "docs_split = text_splitter.split_documents(documents)\n",
        "\n",
        "# def include_doc(doc):\n",
        "#     ## Some chunks will be overburdened with useless numerical data, so we'll filter it out\n",
        "#     string = doc.page_content\n",
        "#     if len([l for l in string if l.isalpha()]) < (len(string)//2):\n",
        "#         return False\n",
        "#     return True\n",
        "\n",
        "# docs_split = [doc for doc in docs_split if include_doc(doc)]\n",
        "print(len(docs_split))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f8bcc89-c781-44d0-9ec1-1fe45eec8b46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f8bcc89-c781-44d0-9ec1-1fe45eec8b46",
        "outputId": "1cf24605-65bb-40a2-e7aa-e2d9a8fb6382"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From Local to Global: A Graph RAG Approach to\n",
            "Query-Focused Summarization\n",
            "Darren Edge1†\n",
            "Ha Trinh1†\n",
            "Newman Cheng2\n",
            "Joshua Bradley2\n",
            "Alex Chao3\n",
            "Apurva Mody3\n",
            "Steven Truitt2\n",
            "Jonathan Larson1\n",
            "1Microsoft Research\n",
            "2Microsoft Strategic Missions and Technologies\n",
            "3Microsoft Office of the CTO\n",
            "{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,steventruitt,jolarso}\n",
            "@microsoft.com\n",
            "†These authors contributed equally to this work\n",
            "Abstract\n",
            "The use of retrieval-augmented generation (RAG) to retrieve relevant informa-\n",
            "tion from an external knowledge source enables large language models (LLMs)\n",
            "to answer questions over private and/or previously unseen document collections.\n",
            "However, RAG fails on global questions directed at an entire text corpus, such\n",
            "as “What are the main themes in the dataset?”, since this is inherently a query-\n",
            "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
            "QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical\n",
            "RAG systems. To combine the strengths of these contrasting methods, we propose\n",
            "a Graph RAG approach to question answering over private text corpora that scales\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a Graph RAG approach to question answering over private text corpora that scales\n",
            "with both the generality of user questions and the quantity of source text to be in-\n",
            "dexed. Our approach uses an LLM to build a graph-based text index in two stages:\n",
            "first to derive an entity knowledge graph from the source documents, then to pre-\n",
            "generate community summaries for all groups of closely-related entities. Given a\n",
            "question, each community summary is used to generate a partial response, before\n",
            "all partial responses are again summarized in a final response to the user. For a\n",
            "class of global sensemaking questions over datasets in the 1 million token range,\n",
            "we show that Graph RAG leads to substantial improvements over a na¨\n",
            "ıve RAG\n",
            "baseline for both the comprehensiveness and diversity of generated answers. An\n",
            "open-source, Python-based implementation of both global and local Graph RAG\n",
            "approaches is forthcoming at https://aka.ms/graphrag.\n",
            "1\n",
            "Introduction\n",
            "Human endeavors across a range of domains rely on our ability to read and reason about large\n",
            "collections of documents, often reaching conclusions that go beyond anything stated in the source\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "collections of documents, often reaching conclusions that go beyond anything stated in the source\n",
            "texts themselves. With the emergence of large language models (LLMs), we are already witnessing\n",
            "attempts to automate human-like sensemaking in complex domains like scientific discovery (Mi-\n",
            "crosoft, 2023) and intelligence analysis (Ranade and Joshi, 2023), where sensemaking is defined as\n",
            "Preprint. Under review.\n",
            "arXiv:2404.16130v1  [cs.CL]  24 Apr 2024\n",
            "Source Documents\n",
            "Text Chunks\n",
            "text extraction\n",
            "and chunking\n",
            "Element Instances\n",
            "domain-tailored\n",
            "summarization\n",
            "Element Summaries\n",
            "domain-tailored\n",
            "summarization\n",
            "Graph Communities\n",
            "community\n",
            "detection\n",
            "Community Summaries\n",
            "domain-tailored\n",
            "summarization\n",
            "Community Answers\n",
            "query-focused\n",
            "summarization\n",
            "Global Answer\n",
            "query-focused\n",
            "summarization\n",
            "Indexing Time\n",
            "Query Time\n",
            "Pipeline Stage\n",
            "Figure 1: Graph RAG pipeline using an LLM-derived graph index of source document text. This\n",
            "index spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that have\n",
            "been detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6\n",
            "Community Summaries →Community Answers →Global Answer\n",
            "Given a user query, the community summaries generated in the previous step can be used to generate\n",
            "a final answer in a multi-stage process. The hierarchical nature of the community structure also\n",
            "means that questions can be answered using the community summaries from different levels, raising\n",
            "the question of whether a particular level in the hierarchical community structure offers the best\n",
            "balance of summary detail and scope for general sensemaking questions (evaluated in section 3).\n",
            "For a given community level, the global answer to any user query is generated as follows:\n",
            "• Prepare community summaries. Community summaries are randomly shuffled and divided\n",
            "into chunks of pre-specified token size. This ensures relevant information is distributed\n",
            "across chunks, rather than concentrated (and potentially lost) in a single context window.\n",
            "• Map community answers. Generate intermediate answers in parallel, one for each chunk.\n",
            "The LLM is also asked to generate a score between 0-100 indicating how helpful the gen-\n",
            "erated answer is in answering the target question. Answers with score 0 are filtered out.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m-1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on\n",
            "Empirical Methods in Natural Language Processing (EMNLP).\n",
            "Yao, J.-g., Wan, X., and Xiao, J. (2017). Recent advances in document summarization. Knowledge\n",
            "and Information Systems, 53:297–336.\n",
            "14\n",
            "Yao, L., Peng, J., Mao, C., and Luo, Y. (2023). Exploring large language models for knowledge\n",
            "graph completion.\n",
            "Zhang, J. (2023). Graph-toolformer: To empower llms with graph reasoning ability via prompt\n",
            "augmented by chatgpt. arXiv preprint arXiv:2304.11116.\n",
            "Zhang, Y., Zhang, Y., Gan, Y., Yao, L., and Wang, C. (2024). Causal graph discovery with retrieval-\n",
            "augmented generation based large language models. arXiv preprint arXiv:2402.15301.\n",
            "Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing,\n",
            "E., et al. (2024). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\n",
            "Information Processing Systems, 36.\n",
            "15\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for i in (0, 1, 2, 15, -1):\n",
        "    pprint(f\"[Document {i}]\")\n",
        "    print(docs_split[i].page_content)\n",
        "    pprint(\"=\"*64)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-0QApYgNbyJD",
      "metadata": {
        "id": "-0QApYgNbyJD"
      },
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## Refining Summaries\n",
        "\n",
        "\n",
        "#### **The DocumentSummaryBase Model**\n",
        "\n",
        "A `DocumentSummaryBase` structure designed to encapsulate the essence of a document. The one below will use the `running_summary` field to query the model for a final summary while attempting to use the `main_ideas` and `loose_ends` fields as a bottleneck to keep the running summary from moving too fast. This is something we're going to have to enforce via prompt engineering, so the `summary_prompt` is also provided which shows how this information will be used. Feel free to modify it as necessary to make it work for your model of choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gE8y2JvLvZ5T",
      "metadata": {
        "id": "gE8y2JvLvZ5T"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import List\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "class DocumentSummaryBase(BaseModel):\n",
        "    running_summary: str = Field(\"\", description=\"Running description of the document. Do not override; only update!\")\n",
        "    main_ideas: List[str] = Field([], description=\"Most important information from the document (max 3)\")\n",
        "    loose_ends: List[str] = Field([], description=\"Open questions that would be good to incorporate into summary, but that are yet unknown (max 3)\")\n",
        "\n",
        "\n",
        "summary_prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are generating a running summary of the document. Make it readable by a technical user.\"\n",
        "    \" After this, the old knowledge base will be replaced by the new one. Make sure a reader can still understand everything.\"\n",
        "    \" Keep it short, but as dense and useful as possible! The information should flow from chunk to (loose ends or main ideas) to running_summary.\"\n",
        "    \" The updated knowledge base keep all of the information from running_summary here: {info_base}.\"\n",
        "    \"\\n\\n{format_instructions}. Follow the format precisely, including quotations and commas\"\n",
        "    \"\\n\\nWithout losing any of the info, update the knowledge base with the following: {input}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "khRhVghHxBaz",
      "metadata": {
        "id": "khRhVghHxBaz"
      },
      "outputs": [],
      "source": [
        "def RExtract(pydantic_class, llm, prompt):\n",
        "    '''\n",
        "    Runnable Extraction module\n",
        "    Returns a knowledge dictionary populated by slot-filling extraction\n",
        "    '''\n",
        "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
        "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
        "    def preparse(string):\n",
        "        if '{' not in string: string = '{' + string\n",
        "        if '}' not in string: string = string + '}'\n",
        "        string = (string\n",
        "            .replace(\"\\\\_\", \"_\")\n",
        "            .replace(\"\\n\", \" \")\n",
        "            .replace(\"\\]\", \"]\")\n",
        "            .replace(\"\\[\", \"[\")\n",
        "        )\n",
        "        # print(string)  ## Good for diagnostics\n",
        "        return string\n",
        "    return instruct_merge | prompt | llm | preparse | parser\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oFtME_s4PRoW",
      "metadata": {
        "id": "oFtME_s4PRoW"
      },
      "source": [
        "<br>\n",
        "\n",
        "With this in mind, the following code invokes the running state chain in a for-loop to iterate over your documents! The only modification necessary should be the `parse_chain` implementation, which should pass the the state through a properly-configured `RExtract` chain. After this, the system should work decently to maintain a running summary of the document (though some tweaking of the prompt may be required depending on the model used)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6sODIfHUgz6m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sODIfHUgz6m",
        "outputId": "7b5aee70-078b-458e-d2a7-e8601b789fb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Considered 15 documents\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"This document discusses the use of retrieval-augmented generation (RAG) for query-focused </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">summarization (QFS) tasks over large text corpora. The authors propose a Graph RAG approach that combines RAG and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">QFS methods, using an LLM to build a graph-based text index in two stages: first to derive an entity knowledge </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">graph, then to pre-generate community summaries for groups of related entities. The approach outperforms naïve RAG </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and global map-reduce summarization on comprehensiveness and diversity. The main ideas include the ability of RAG </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to retrieve relevant information from an external knowledge source, the failure of RAG on global questions, and the</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">benefits of the Graph RAG approach. Open questions remain about the performance of Graph RAG compared to existing </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">methods, the main themes identified in the dataset, and the impact of Graph RAG on understanding broad issues and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">themes. For improving extraction quality, a multi-stage approach is proposed in which the LLM assesses whether all </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">entities were extracted, and if necessary, it gathers missing entities. The use of an LLM to 'extract' descriptions</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">of entities, relationships, and claims is a form of abstractive summarization, and to convert all instance-level </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">summaries into single blocks of descriptive text, a further round of LLM summarization over matching groups of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">instances is required. However, variations in entity naming may result in duplicate nodes in the entity graph. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Despite this, the approach is resilient due to the LLM's ability to understand common entities behind multiple name</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">variations. The use of rich descriptive text differentiates the graph index from typical knowledge graphs. The </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">index created can be modeled as an homogeneous undirected weighted graph in which entity nodes are connected by </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">relationship edges. Various community detection algorithms could be used to partition the graph into communities of</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">nodes with stronger connections (e.g., Leiden). Each level of the hierarchy provides a community partition that </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">covers the nodes of the graph in a mutually-exclusive, collective-exhaustive way, enabling divide-and-conquer </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">global summarization. Finally, report-like summaries of each community are created using a method designed to scale</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to very large datasets. These summaries are independently useful in their own right as a way to understand the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">global structure and semantics of the dataset, and may themselves be used to make sense of a corpus in the absence </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">of a question. For example, a user may scan through community summaries at one level looking for general themes of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">interest, then follow links to the reports at the lower level that provide more details for each of the subtopics. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Here, however, we focus on their utility as part of a graph-based index used for answering global queries. For </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">hierarchical clustering, two levels are considered: (a) Level 0, corresponding to the hierarchical partition with </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">maximum modularity, and (b) Level 1, which reveals internal structure within these root-level communities. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Leaf-level communities are summarized by prioritizing and iteratively adding element summaries to the LLM context </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">window until the token limit is reached. Prioritization is done by adding descriptions of source node, target node,</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">linked covariates, and the edge itself in decreasing order of combined source and target node degree. Higher-level </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">communities are handled similarly, but if all element summaries do not fit within the token limit, rank </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">sub-communities in decreasing order of element summary tokens and iteratively substitute sub-community summaries </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">for associated element summaries until they fit within the context window.\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'The use of RAG to retrieve relevant information from an external knowledge source enables LLMs to answer </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">questions over private and/or previously unseen document collections.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'RAG fails on global questions directed at an entire text corpus, since this is inherently a QFS task, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">rather than an explicit retrieval task.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'The authors propose a Graph RAG approach to question answering over private text corpora that scales with </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">both the generality of user questions and the quantity of source text to be indexed. The approach uses an LLM to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">then to pre-generate community summaries for all groups of closely-related entities. The proposed Graph RAG </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">approach outperforms naïve RAG and global map-reduce summarization on comprehensiveness and diversity.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does the proposed Graph RAG approach perform compared to existing methods?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What are the main themes identified in the dataset using the Graph RAG approach?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does Graph RAG contribute to human endeavors across a range of domains by enabling better reading and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning about large collections of documents?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How can the challenge of query-focused abstractive summarization over an entire corpus be addressed, given</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">that such volumes of text can greatly exceed the limits of LLM context windows?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does the approach impact the target qualities of comprehensiveness, diversity, and empowerment that </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">develop understanding of broad issues and themes?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How to achieve the balance of recall and precision in the extraction process for the target activity?'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"This\u001b[0m\u001b[32m document discusses the use of retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for query-focused \u001b[0m\n",
              "\u001b[32msummarization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m tasks over large text corpora. The authors propose a Graph RAG approach that combines RAG and \u001b[0m\n",
              "\u001b[32mQFS methods, using an LLM to build a graph-based text index in two stages: first to derive an entity knowledge \u001b[0m\n",
              "\u001b[32mgraph, then to pre-generate community summaries for groups of related entities. The approach outperforms naïve RAG \u001b[0m\n",
              "\u001b[32mand global map-reduce summarization on comprehensiveness and diversity. The main ideas include the ability of RAG \u001b[0m\n",
              "\u001b[32mto retrieve relevant information from an external knowledge source, the failure of RAG on global questions, and the\u001b[0m\n",
              "\u001b[32mbenefits of the Graph RAG approach. Open questions remain about the performance of Graph RAG compared to existing \u001b[0m\n",
              "\u001b[32mmethods, the main themes identified in the dataset, and the impact of Graph RAG on understanding broad issues and \u001b[0m\n",
              "\u001b[32mthemes. For improving extraction quality, a multi-stage approach is proposed in which the LLM assesses whether all \u001b[0m\n",
              "\u001b[32mentities were extracted, and if necessary, it gathers missing entities. The use of an LLM to 'extract' descriptions\u001b[0m\n",
              "\u001b[32mof entities, relationships, and claims is a form of abstractive summarization, and to convert all instance-level \u001b[0m\n",
              "\u001b[32msummaries into single blocks of descriptive text, a further round of LLM summarization over matching groups of \u001b[0m\n",
              "\u001b[32minstances is required. However, variations in entity naming may result in duplicate nodes in the entity graph. \u001b[0m\n",
              "\u001b[32mDespite this, the approach is resilient due to the LLM's ability to understand common entities behind multiple name\u001b[0m\n",
              "\u001b[32mvariations. The use of rich descriptive text differentiates the graph index from typical knowledge graphs. The \u001b[0m\n",
              "\u001b[32mindex created can be modeled as an homogeneous undirected weighted graph in which entity nodes are connected by \u001b[0m\n",
              "\u001b[32mrelationship edges. Various community detection algorithms could be used to partition the graph into communities of\u001b[0m\n",
              "\u001b[32mnodes with stronger connections \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Each level of the hierarchy provides a community partition that \u001b[0m\n",
              "\u001b[32mcovers the nodes of the graph in a mutually-exclusive, collective-exhaustive way, enabling divide-and-conquer \u001b[0m\n",
              "\u001b[32mglobal summarization. Finally, report-like summaries of each community are created using a method designed to scale\u001b[0m\n",
              "\u001b[32mto very large datasets. These summaries are independently useful in their own right as a way to understand the \u001b[0m\n",
              "\u001b[32mglobal structure and semantics of the dataset, and may themselves be used to make sense of a corpus in the absence \u001b[0m\n",
              "\u001b[32mof a question. For example, a user may scan through community summaries at one level looking for general themes of \u001b[0m\n",
              "\u001b[32minterest, then follow links to the reports at the lower level that provide more details for each of the subtopics. \u001b[0m\n",
              "\u001b[32mHere, however, we focus on their utility as part of a graph-based index used for answering global queries. For \u001b[0m\n",
              "\u001b[32mhierarchical clustering, two levels are considered: \u001b[0m\u001b[32m(\u001b[0m\u001b[32ma\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Level 0, corresponding to the hierarchical partition with \u001b[0m\n",
              "\u001b[32mmaximum modularity, and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Level 1, which reveals internal structure within these root-level communities. \u001b[0m\n",
              "\u001b[32mLeaf-level communities are summarized by prioritizing and iteratively adding element summaries to the LLM context \u001b[0m\n",
              "\u001b[32mwindow until the token limit is reached. Prioritization is done by adding descriptions of source node, target node,\u001b[0m\n",
              "\u001b[32mlinked covariates, and the edge itself in decreasing order of combined source and target node degree. Higher-level \u001b[0m\n",
              "\u001b[32mcommunities are handled similarly, but if all element summaries do not fit within the token limit, rank \u001b[0m\n",
              "\u001b[32msub-communities in decreasing order of element summary tokens and iteratively substitute sub-community summaries \u001b[0m\n",
              "\u001b[32mfor associated element summaries until they fit within the context window.\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'The use of RAG to retrieve relevant information from an external knowledge source enables LLMs to answer \u001b[0m\n",
              "\u001b[32mquestions over private and/or previously unseen document collections.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'RAG fails on global questions directed at an entire text corpus, since this is inherently a QFS task, \u001b[0m\n",
              "\u001b[32mrather than an explicit retrieval task.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'The authors propose a Graph RAG approach to question answering over private text corpora that scales with \u001b[0m\n",
              "\u001b[32mboth the generality of user questions and the quantity of source text to be indexed. The approach uses an LLM to \u001b[0m\n",
              "\u001b[32mbuild a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, \u001b[0m\n",
              "\u001b[32mthen to pre-generate community summaries for all groups of closely-related entities. The proposed Graph RAG \u001b[0m\n",
              "\u001b[32mapproach outperforms naïve RAG and global map-reduce summarization on comprehensiveness and diversity.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does the proposed Graph RAG approach perform compared to existing methods?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What are the main themes identified in the dataset using the Graph RAG approach?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does Graph RAG contribute to human endeavors across a range of domains by enabling better reading and \u001b[0m\n",
              "\u001b[32mreasoning about large collections of documents?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How can the challenge of query-focused abstractive summarization over an entire corpus be addressed, given\u001b[0m\n",
              "\u001b[32mthat such volumes of text can greatly exceed the limits of LLM context windows?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does the approach impact the target qualities of comprehensiveness, diversity, and empowerment that \u001b[0m\n",
              "\u001b[32mdevelop understanding of broad issues and themes?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How to achieve the balance of recall and precision in the extraction process for the target activity?'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "latest_summary = \"\"\n",
        "\n",
        "## TODO: Use the techniques from the previous notebook to complete the exercise\n",
        "def RSummarizer(knowledge, llm, prompt, verbose=False):\n",
        "    '''\n",
        "    Exercise: Create a chain that summarizes\n",
        "    '''\n",
        "    ###########################################################################################\n",
        "    ## START TODO:\n",
        "\n",
        "    def summarize_docs(docs):\n",
        "        ## TODO: Initialize the parse_chain appropriately; should include an RExtract instance.\n",
        "        ## HINT: You can get a class using the <object>.__class__ attribute...\n",
        "        parse_chain = RunnableAssign({'info_base' : RExtract(knowledge.__class__, llm, prompt)})\n",
        "\n",
        "        ## TODO: Initialize a valid starting state. Should be similar to notebook 4\n",
        "        state = {'info_base' : knowledge}\n",
        "\n",
        "        global latest_summary  ## If your loop crashes, you can check out the latest_summary\n",
        "\n",
        "        for i, doc in enumerate(docs):\n",
        "            ## TODO: Update the state as appropriate using your parse_chain component\n",
        "            state['input'] = doc.page_content\n",
        "            state = parse_chain.invoke(state)\n",
        "\n",
        "            assert 'info_base' in state\n",
        "            if verbose:\n",
        "                print(f\"Considered {i+1} documents\")\n",
        "                pprint(state['info_base'])\n",
        "                latest_summary = state['info_base']\n",
        "                clear_output(wait=True)\n",
        "\n",
        "        return state['info_base']\n",
        "\n",
        "    ## END TODO\n",
        "    ###########################################################################################\n",
        "\n",
        "    return RunnableLambda(summarize_docs)\n",
        "\n",
        "# instruct_model = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\").bind(max_tokens=4096)\n",
        "instruct_model = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\").bind(max_tokens=4096)\n",
        "instruct_llm = instruct_model | StrOutputParser()\n",
        "\n",
        "## Take the first 10 document chunks and accumulate a DocumentSummaryBase\n",
        "summarizer = RSummarizer(DocumentSummaryBase(), instruct_llm, summary_prompt, verbose=True)\n",
        "summary = summarizer.invoke(docs_split[:15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07eb5710-23f7-4782-84eb-1fc8f73500b2",
      "metadata": {
        "id": "07eb5710-23f7-4782-84eb-1fc8f73500b2",
        "outputId": "17204b7a-d6c8-44da-9a45-6ff9ba967cf9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"This document discusses the use of retrieval-augmented generation (RAG) for query-focused </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">summarization (QFS) tasks over large text corpora. The authors propose a Graph RAG approach that combines RAG and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">QFS methods, using an LLM to build a graph-based text index in two stages: first to derive an entity knowledge </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">graph, then to pre-generate community summaries for groups of related entities. The approach outperforms naïve RAG </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and global map-reduce summarization on comprehensiveness and diversity. The main ideas include the ability of RAG </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to retrieve relevant information from an external knowledge source, the failure of RAG on global questions, and the</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">benefits of the Graph RAG approach. Open questions remain about the performance of Graph RAG compared to existing </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">methods, the main themes identified in the dataset, and the impact of Graph RAG on understanding broad issues and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">themes. For improving extraction quality, a multi-stage approach is proposed in which the LLM assesses whether all </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">entities were extracted, and if necessary, it gathers missing entities. The use of an LLM to 'extract' descriptions</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">of entities, relationships, and claims is a form of abstractive summarization, and to convert all instance-level </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">summaries into single blocks of descriptive text, a further round of LLM summarization over matching groups of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">instances is required. However, variations in entity naming may result in duplicate nodes in the entity graph. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Despite this, the approach is resilient due to the LLM's ability to understand common entities behind multiple name</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">variations. The use of rich descriptive text differentiates the graph index from typical knowledge graphs. The </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">index created can be modeled as an homogeneous undirected weighted graph in which entity nodes are connected by </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">relationship edges. Various community detection algorithms could be used to partition the graph into communities of</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">nodes with stronger connections (e.g., Leiden). Each level of the hierarchy provides a community partition that </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">covers the nodes of the graph in a mutually-exclusive, collective-exhaustive way, enabling divide-and-conquer </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">global summarization. Finally, report-like summaries of each community are created using a method designed to scale</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to very large datasets. These summaries are independently useful in their own right as a way to understand the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">global structure and semantics of the dataset, and may themselves be used to make sense of a corpus in the absence </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">of a question. For example, a user may scan through community summaries at one level looking for general themes of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">interest, then follow links to the reports at the lower level that provide more details for each of the subtopics. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Here, however, we focus on their utility as part of a graph-based index used for answering global queries. For </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">hierarchical clustering, two levels are considered: (a) Level 0, corresponding to the hierarchical partition with </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">maximum modularity, and (b) Level 1, which reveals internal structure within these root-level communities. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Leaf-level communities are summarized by prioritizing and iteratively adding element summaries to the LLM context </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">window until the token limit is reached. Prioritization is done by adding descriptions of source node, target node,</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">linked covariates, and the edge itself in decreasing order of combined source and target node degree. Higher-level </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">communities are handled similarly, but if all element summaries do not fit within the token limit, rank </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">sub-communities in decreasing order of element summary tokens and iteratively substitute sub-community summaries </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">for associated element summaries until they fit within the context window.\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'The use of RAG to retrieve relevant information from an external knowledge source enables LLMs to answer </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">questions over private and/or previously unseen document collections.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'RAG fails on global questions directed at an entire text corpus, since this is inherently a QFS task, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">rather than an explicit retrieval task.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'The authors propose a Graph RAG approach to question answering over private text corpora that scales with </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">both the generality of user questions and the quantity of source text to be indexed. The approach uses an LLM to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">then to pre-generate community summaries for all groups of closely-related entities. The proposed Graph RAG </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">approach outperforms naïve RAG and global map-reduce summarization on comprehensiveness and diversity.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does the proposed Graph RAG approach perform compared to existing methods?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What are the main themes identified in the dataset using the Graph RAG approach?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does Graph RAG contribute to human endeavors across a range of domains by enabling better reading and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning about large collections of documents?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How can the challenge of query-focused abstractive summarization over an entire corpus be addressed, given</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">that such volumes of text can greatly exceed the limits of LLM context windows?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does the approach impact the target qualities of comprehensiveness, diversity, and empowerment that </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">develop understanding of broad issues and themes?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How to achieve the balance of recall and precision in the extraction process for the target activity?'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"This\u001b[0m\u001b[32m document discusses the use of retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for query-focused \u001b[0m\n",
              "\u001b[32msummarization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m tasks over large text corpora. The authors propose a Graph RAG approach that combines RAG and \u001b[0m\n",
              "\u001b[32mQFS methods, using an LLM to build a graph-based text index in two stages: first to derive an entity knowledge \u001b[0m\n",
              "\u001b[32mgraph, then to pre-generate community summaries for groups of related entities. The approach outperforms naïve RAG \u001b[0m\n",
              "\u001b[32mand global map-reduce summarization on comprehensiveness and diversity. The main ideas include the ability of RAG \u001b[0m\n",
              "\u001b[32mto retrieve relevant information from an external knowledge source, the failure of RAG on global questions, and the\u001b[0m\n",
              "\u001b[32mbenefits of the Graph RAG approach. Open questions remain about the performance of Graph RAG compared to existing \u001b[0m\n",
              "\u001b[32mmethods, the main themes identified in the dataset, and the impact of Graph RAG on understanding broad issues and \u001b[0m\n",
              "\u001b[32mthemes. For improving extraction quality, a multi-stage approach is proposed in which the LLM assesses whether all \u001b[0m\n",
              "\u001b[32mentities were extracted, and if necessary, it gathers missing entities. The use of an LLM to 'extract' descriptions\u001b[0m\n",
              "\u001b[32mof entities, relationships, and claims is a form of abstractive summarization, and to convert all instance-level \u001b[0m\n",
              "\u001b[32msummaries into single blocks of descriptive text, a further round of LLM summarization over matching groups of \u001b[0m\n",
              "\u001b[32minstances is required. However, variations in entity naming may result in duplicate nodes in the entity graph. \u001b[0m\n",
              "\u001b[32mDespite this, the approach is resilient due to the LLM's ability to understand common entities behind multiple name\u001b[0m\n",
              "\u001b[32mvariations. The use of rich descriptive text differentiates the graph index from typical knowledge graphs. The \u001b[0m\n",
              "\u001b[32mindex created can be modeled as an homogeneous undirected weighted graph in which entity nodes are connected by \u001b[0m\n",
              "\u001b[32mrelationship edges. Various community detection algorithms could be used to partition the graph into communities of\u001b[0m\n",
              "\u001b[32mnodes with stronger connections \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Each level of the hierarchy provides a community partition that \u001b[0m\n",
              "\u001b[32mcovers the nodes of the graph in a mutually-exclusive, collective-exhaustive way, enabling divide-and-conquer \u001b[0m\n",
              "\u001b[32mglobal summarization. Finally, report-like summaries of each community are created using a method designed to scale\u001b[0m\n",
              "\u001b[32mto very large datasets. These summaries are independently useful in their own right as a way to understand the \u001b[0m\n",
              "\u001b[32mglobal structure and semantics of the dataset, and may themselves be used to make sense of a corpus in the absence \u001b[0m\n",
              "\u001b[32mof a question. For example, a user may scan through community summaries at one level looking for general themes of \u001b[0m\n",
              "\u001b[32minterest, then follow links to the reports at the lower level that provide more details for each of the subtopics. \u001b[0m\n",
              "\u001b[32mHere, however, we focus on their utility as part of a graph-based index used for answering global queries. For \u001b[0m\n",
              "\u001b[32mhierarchical clustering, two levels are considered: \u001b[0m\u001b[32m(\u001b[0m\u001b[32ma\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Level 0, corresponding to the hierarchical partition with \u001b[0m\n",
              "\u001b[32mmaximum modularity, and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Level 1, which reveals internal structure within these root-level communities. \u001b[0m\n",
              "\u001b[32mLeaf-level communities are summarized by prioritizing and iteratively adding element summaries to the LLM context \u001b[0m\n",
              "\u001b[32mwindow until the token limit is reached. Prioritization is done by adding descriptions of source node, target node,\u001b[0m\n",
              "\u001b[32mlinked covariates, and the edge itself in decreasing order of combined source and target node degree. Higher-level \u001b[0m\n",
              "\u001b[32mcommunities are handled similarly, but if all element summaries do not fit within the token limit, rank \u001b[0m\n",
              "\u001b[32msub-communities in decreasing order of element summary tokens and iteratively substitute sub-community summaries \u001b[0m\n",
              "\u001b[32mfor associated element summaries until they fit within the context window.\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'The use of RAG to retrieve relevant information from an external knowledge source enables LLMs to answer \u001b[0m\n",
              "\u001b[32mquestions over private and/or previously unseen document collections.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'RAG fails on global questions directed at an entire text corpus, since this is inherently a QFS task, \u001b[0m\n",
              "\u001b[32mrather than an explicit retrieval task.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'The authors propose a Graph RAG approach to question answering over private text corpora that scales with \u001b[0m\n",
              "\u001b[32mboth the generality of user questions and the quantity of source text to be indexed. The approach uses an LLM to \u001b[0m\n",
              "\u001b[32mbuild a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, \u001b[0m\n",
              "\u001b[32mthen to pre-generate community summaries for all groups of closely-related entities. The proposed Graph RAG \u001b[0m\n",
              "\u001b[32mapproach outperforms naïve RAG and global map-reduce summarization on comprehensiveness and diversity.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does the proposed Graph RAG approach perform compared to existing methods?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What are the main themes identified in the dataset using the Graph RAG approach?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does Graph RAG contribute to human endeavors across a range of domains by enabling better reading and \u001b[0m\n",
              "\u001b[32mreasoning about large collections of documents?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How can the challenge of query-focused abstractive summarization over an entire corpus be addressed, given\u001b[0m\n",
              "\u001b[32mthat such volumes of text can greatly exceed the limits of LLM context windows?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does the approach impact the target qualities of comprehensiveness, diversity, and empowerment that \u001b[0m\n",
              "\u001b[32mdevelop understanding of broad issues and themes?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How to achieve the balance of recall and precision in the extraction process for the target activity?'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pprint(latest_summary)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}