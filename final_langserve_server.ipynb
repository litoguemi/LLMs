{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "38ee3921-2244-4545-b0df-0b0ebebff32d",
      "metadata": {
        "id": "38ee3921-2244-4545-b0df-0b0ebebff32d"
      },
      "source": [
        "<br>\n",
        "\n",
        "# <font color=\"#76b900\">LangServe Routes</font>\n",
        "\n",
        "<br>\n",
        "\n",
        "## LangServe Server Setup\n",
        "\n",
        "This notebook is playground for those interested in developing interactive web applications using LangChain and [**LangServe**](https://python.langchain.com/docs/langserve). The aim is to provide a minimal-code example to illustrate the potential of LangChain in web application contexts.\n",
        "\n",
        "This section provides a walkthrough for setting up a simple API server using LangChain's Runnable interfaces with FastAPI. The example demonstrates how to integrate a LangChain model, such as `ChatNVIDIA`, to create and distribute accessible API routes. Using this, you will be able to supply functionality to the frontend service's [**`frontend_server.py`**](frontend/frontend_server.py) session, which strongly expects:\n",
        "- A simple endpoint named `:9012/basic_chat` for the basic chatbot, exemplified below.\n",
        "- A pair of endpoints named `:9012/retriever` and `:9012/generator` for the RAG chatbot.\n",
        "- All three for the **Evaluate** utility, which will be required for the final assessment. *More on that later!*\n",
        "\n",
        "**IMPORTANT NOTES:**\n",
        "- Make sure to shut down an active FastAPI cell. The first time might fall through or trigger a try-catch routine on an asynchronous process.\n",
        "- If it still doesn't work, do a hard restart on this notebook by using **Kernel -> Restart Kernel**.\n",
        "- When a FastAPI server is running in your cell, expect the process to block up this notebook. Other notebooks should not be impacted by this."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YRX1R3GupzkZ",
      "metadata": {
        "id": "YRX1R3GupzkZ"
      },
      "source": [
        "<br>\n",
        "\n",
        "### **Part 1:** Delivering the /basic_chat endpoint\n",
        "\n",
        "Instructions are provided for launching a `/basic_chat` endpoint both as a standalone Python file. This will be used by the frontend to make basic decision with no internal reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TniVLtL-qcqo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TniVLtL-qcqo",
        "outputId": "7ff6eb58-b9c1-4ce9-b15a-b1a515045ae0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting server_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile server_app.py\n",
        "# https://python.langchain.com/docs/langserve#server\n",
        "from fastapi import FastAPI\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "from langserve import add_routes\n",
        "\n",
        "## May be useful later\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.prompt_values import ChatPromptValue\n",
        "from langchain_core.runnables import RunnableLambda, RunnableBranch, RunnablePassthrough\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "from langchain_community.document_transformers import LongContextReorder\n",
        "from functools import partial\n",
        "from operator import itemgetter\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "## TODO: Make sure to pick your LLM and do your prompt engineering\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "\n",
        "instruct_llm = ChatNVIDIA(model=\"meta/llama3-8b-instruct\")\n",
        "llm = instruct_llm | StrOutputParser()\n",
        "\n",
        "\n",
        "def docs2str(docs, title=\"Document\"):\n",
        "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
        "    out_str = \"\"\n",
        "    for doc in docs:\n",
        "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
        "        if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
        "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
        "    return out_str\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
        "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
        "    \" The following information may be useful for your response: \"\n",
        "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
        "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
        "    \"\\n\\nUser Question: {input}\"\n",
        ")\n",
        "\n",
        "def output_puller(inputs):\n",
        "    \"\"\"\"Output generator. Useful if your chain returns a dictionary with key 'output'\"\"\"\n",
        "    if isinstance(inputs, dict):\n",
        "        inputs = [inputs]\n",
        "    for token in inputs:\n",
        "        if token.get('output'):\n",
        "            yield token.get('output')\n",
        "\n",
        "app = FastAPI(\n",
        "  title=\"LangChain Server\",\n",
        "  version=\"1.0\",\n",
        "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
        ")\n",
        "\n",
        "\n",
        "def output_puller(inputs):\n",
        "    \"\"\"Output generator. Handles various input types.\"\"\"\n",
        "    if isinstance(inputs, dict):\n",
        "        return inputs.get('output', '')\n",
        "    elif isinstance(inputs, str):\n",
        "        return inputs\n",
        "    elif isinstance(inputs, list):\n",
        "        return ' '.join(str(item) for item in inputs)\n",
        "    else:\n",
        "        return str(inputs)\n",
        "\n",
        "generator_chain = chat_prompt | llm\n",
        "generator_chain = {\"output\": generator_chain} | RunnableLambda(output_puller)\n",
        "\n",
        "add_routes(\n",
        "    app,\n",
        "    generator_chain,\n",
        "    path=\"/generator\",\n",
        ")\n",
        "\n",
        "docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
        "long_reorder = RunnableLambda(LongContextReorder().transform_documents)  ## GIVEN\n",
        "\n",
        "context_getter = itemgetter('input') | docstore.as_retriever() | long_reorder | docs2str\n",
        "retrieval_chain = {'input' : (lambda x: x)} | RunnableAssign({'context' : context_getter})\n",
        "\n",
        "add_routes(\n",
        "    app,\n",
        "    retrieval_chain,\n",
        "    path=\"/retriever\",\n",
        ")\n",
        "\n",
        "\n",
        "##  Run as-is and see the basic chain in action\n",
        "rag_chain = retrieval_chain | generator_chain\n",
        "\n",
        "add_routes(\n",
        "    app,\n",
        "    llm,\n",
        "    path=\"/basic_chat\",\n",
        ")\n",
        "\n",
        "\n",
        "## Might be encountered if this were for a standalone python file...\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=9012)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u2xDAYn1qi_D",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2xDAYn1qi_D",
        "outputId": "ef35c8f4-210c-4c10-82e5-a3de2bfe1835"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m1474\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\n",
            " __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______\n",
            "|  |        /   \\     |  \\ |  |  /  _____|    /       ||   ____||   _  \\    \\   \\  /   / |   ____|\n",
            "|  |       /  ^  \\    |   \\|  | |  |  __     |   (----`|  |__   |  |_)  |    \\   \\/   /  |  |__\n",
            "|  |      /  /_\\  \\   |  . `  | |  | |_ |     \\   \\    |   __|  |      /      \\      /   |   __|\n",
            "|  `----./  _____  \\  |  |\\   | |  |__| | .----)   |   |  |____ |  |\\  \\----.  \\    /    |  |____\n",
            "|_______/__/     \\__\\ |__| \\__|  \\______| |_______/    |_______|| _| `._____|   \\__/     |_______|\n",
            "\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/retriever/\" is live at:\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m  │\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m  └──> /retriever/playground/\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/basic_chat/\" is live at:\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m  │\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m  └──> /basic_chat/playground/\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/generator/\" is live at:\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m  │\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m  └──> /generator/playground/\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
            "\u001b[1;32;40mLANGSERVE:\u001b[0m See all available routes at /docs/\n",
            "\n",
            "\u001b[1;31;40mLANGSERVE:\u001b[0m ⚠️ Using pydantic 2.8.2. OpenAPI docs for invoke, batch, stream, stream_log endpoints will not be generated. API endpoints and playground should work as expected. If you need to see the docs, you can downgrade to pydantic 1. For example, `pip install pydantic==1.10.13`. See https://github.com/tiangolo/fastapi/issues/10360 for details.\n",
            "\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:9012\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     172.18.0.7:46510 - \"\u001b[1mPOST /basic_chat/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     172.18.0.7:46526 - \"\u001b[1mPOST /retriever/stream HTTP/1.1\u001b[0m\" \u001b[91m500 Internal Server Error\u001b[0m\n",
            "\u001b[31mERROR\u001b[0m:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/site-packages/pydantic/v1/main.py\", line 716, in validate\n",
            "    value_as_dict = dict(value)\n",
            "                    ^^^^^^^^^^^\n",
            "ValueError: dictionary update sequence element #0 has length 1; 2 is required\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 411, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 756, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 776, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 297, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 77, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 72, in app\n",
            "    response = await func(request)\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/fastapi/routing.py\", line 278, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/langserve/server.py\", line 582, in stream\n",
            "    return await api_handler.stream(request)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/langserve/api_handler.py\", line 1086, in stream\n",
            "    config, input_ = await self._get_config_and_input(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/langserve/api_handler.py\", line 778, in _get_config_and_input\n",
            "    input_ = schema.validate(body.input)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/pydantic/v1/main.py\", line 718, in validate\n",
            "    raise DictError() from e\n",
            "pydantic.v1.errors.DictError: value is not a valid dict\n",
            "\u001b[32mINFO\u001b[0m:     172.18.0.7:46532 - \"\u001b[1mPOST /generator/stream HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "## Works, but will block the notebook.\n",
        "!python server_app.py\n",
        "\n",
        "## Will technically work, but not recommended in a notebook.\n",
        "## You may be surprised at the interesting side effects...\n",
        "# import os\n",
        "# os.system(\"python server_app.py &\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g9uRMEOrsy1d",
      "metadata": {
        "id": "g9uRMEOrsy1d"
      },
      "source": [
        "<br>\n",
        "\n",
        "### **Part 2:** Using The Server:\n",
        "\n",
        "While this cannot be easily utilized within Google Colab (or at least not without a lot of special tricks), the above script will keep a running server tied to the notebook process. While the server is running, do not attempt to use this notebook (except to shut down/restart the service).\n",
        "\n",
        "In another file, however, you should be able to access the `basic_chat` endpoint using the following interface:\n",
        "\n",
        "```python\n",
        "from langserve import RemoteRunnable\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = RemoteRunnable(\"http://0.0.0.0:9012/basic_chat/\") | StrOutputParser()\n",
        "for token in llm.stream(\"Hello World! How is it going?\"):\n",
        "    print(token, end='')\n",
        "```\n",
        "\n",
        "**Please try it out in a different file and see if it works!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a1d5501-b916-4045-bb61-b9f35ecad5df",
      "metadata": {
        "id": "6a1d5501-b916-4045-bb61-b9f35ecad5df"
      },
      "source": [
        "<br>\n",
        "\n",
        "### **Part 3:** Final\n",
        "\n",
        "For a quick link to the frontend, run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24beaf98-fb5e-477d-a711-5204f1cb4057",
      "metadata": {
        "id": "24beaf98-fb5e-477d-a711-5204f1cb4057",
        "outputId": "c579d343-5c47-4eec-f73d-ac5b0c18c3b6"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "var url = 'http://'+window.location.host+':8090';\n",
              "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%js\n",
        "var url = 'http://'+window.location.host+':8090';\n",
        "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49c72a8e-3b5b-4442-a6aa-b94b839cacb2",
      "metadata": {
        "id": "49c72a8e-3b5b-4442-a6aa-b94b839cacb2"
      },
      "source": [
        "----"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}